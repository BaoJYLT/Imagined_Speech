import os
import numpy as np
import pywt
import matplotlib.pyplot as plt
from scipy.signal import butter, filtfilt
import os
import numpy as np
import pywt
import matplotlib.pyplot as plt
from scipy.signal import butter, filtfilt
import os
import numpy as np
import pywt
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import seaborn as sns
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
import pandas as pd
import random
import os
import numpy as np
import pywt
import matplotlib.pyplot as plt
from scipy.signal import butter, filtfilt
import os
import numpy as np
import pywt
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import seaborn as sns
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import *
import pandas as pd

# %%é¢„å¤„ç†

# è¤ç«è™«ä¼˜åŒ–ç®—æ³•ï¼ˆä¼˜åŒ–è½¯é˜ˆå€¼ï¼‰â€”â€”ç”¨äºæ•°æ®é¢„å¤„ç†
def firefly_algorithm(detail_coeffs, n_fireflies=15, max_iter=20, alpha=0.2, beta0=1, gamma=1):
    def fitness(threshold):
        thresholded = pywt.threshold(detail_coeffs, threshold, mode='soft')
        return -np.var(thresholded)  # è¶Šå¹³æ»‘è¶Šå¥½ï¼ˆè´Ÿæ–¹å·®ï¼‰

    thresholds = np.random.uniform(0.01, np.max(np.abs(detail_coeffs)), n_fireflies)
    fitness_vals = np.array([fitness(th) for th in thresholds])

    for t in range(max_iter):
        for i in range(n_fireflies):
            for j in range(n_fireflies):
                if fitness_vals[j] > fitness_vals[i]:
                    r = np.abs(thresholds[i] - thresholds[j])
                    beta = beta0 * np.exp(-gamma * r**2)
                    thresholds[i] = thresholds[i] + beta * (thresholds[j] - thresholds[i]) + alpha * (np.random.rand() - 0.5)
        fitness_vals = np.array([fitness(th) for th in thresholds])

    best_idx = np.argmax(fitness_vals)
    return thresholds[best_idx]

# å°æ³¢å»å™ªï¼ˆæ¯ä¸ªé€šé“åº”ç”¨ FOA-soft é˜ˆå€¼ï¼‰
def wavelet_denoise(signal, wavelet='db4', level=4):
    coeffs = pywt.wavedec(signal, wavelet, level=level)
    thresholded_coeffs = [coeffs[0]]
    for detail in coeffs[1:]:
        th = firefly_algorithm(detail)
        thresholded = pywt.threshold(detail, th, mode='soft')
        thresholded_coeffs.append(thresholded)
    return pywt.waverec(thresholded_coeffs, wavelet)

# EEG ä¿¡å·é¢„å¤„ç†å‡½æ•°ï¼ˆæ»¤æ³¢ + FOA-DWT å»å™ª + Z-scoreï¼‰
# ä¿®æ”¹ preprocess_file å‡½æ•°ä»¥æ·»åŠ æ‰“å°è¿‡ç¨‹ä¸æ›´å¤šå¯è§†åŒ–ï¼Œè¿”å›çš„æ˜¯processedçš„æ•°æ®å’Œlabels
# EEG ä¿¡å·é¢„å¤„ç†å‡½æ•°ï¼ˆæ»¤æ³¢ + FOA-DWT å»å™ª + Z-scoreï¼‰
def preprocess_file(data_path, label_path, out_data_path, out_label_path, labels_to_select, label_mapping, fs=256, lowcut=0.5, highcut=50.0):
    print(f"åŠ è½½æ•°æ®ï¼š{data_path}")
    data = np.load(data_path)  # (n_trials, C, T)
    labels = np.load(label_path)
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶ï¼š{data.shape}ï¼Œæ ‡ç­¾å½¢çŠ¶ï¼š{labels.shape}")

    # ===== æ ‡ç­¾ç­›é€‰ =====
    selected_mask = np.isin(labels, labels_to_select)
    data = data[selected_mask]
    labels = labels[selected_mask]

    # æ ‡ç­¾æ˜ å°„
    labels = np.vectorize(label_mapping.get)(labels)
    print(f"ç­›é€‰åçš„æ ·æœ¬æ•°ï¼š{len(labels)}")

    b, a = butter(N=4, Wn=[lowcut / (fs / 2), highcut / (fs / 2)], btype='band')
    start_idx, end_idx = int(1.0 * fs), int(3.0 * fs)

    n_trials, C, T = data.shape
    processed = np.zeros((n_trials, C, end_idx - start_idx), dtype=np.float32)

    for i in range(n_trials):
        print(f"é¢„å¤„ç†ç¬¬ {i+1}/{n_trials} ä¸ªè¯•æ¬¡...")
        segment = data[i][:, start_idx:end_idx]
        filtered = filtfilt(b, a, segment, axis=1)
        denoised = np.array([wavelet_denoise(filtered[ch])[:end_idx-start_idx] for ch in range(C)])
        zscored = (denoised - denoised.mean()) / (denoised.std() + 1e-8)
        processed[i] = zscored

    print(f"ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®åˆ°ï¼š{out_data_path}")
    np.save(out_data_path, processed)
    np.save(out_label_path, labels)
    print("å¤„ç†å®Œæˆã€‚\n")
    return processed, labels

# å¯è§†åŒ–å‡½æ•°ï¼ˆé»˜è®¤å±•ç¤º Trial 0, Channel 10ï¼‰
def plot_example_signal(data, trial_idx=0, channel_idx=10, title="Preprocessed EEG with FOA-DWT"):
    plt.figure(figsize=(10, 4))
    plt.plot(data[trial_idx, channel_idx], label=f"Trial {trial_idx}, Channel {channel_idx}")
    plt.title(title)
    plt.xlabel("Sample Index")
    plt.ylabel("Amplitude (z-scored)")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()


# %% æ•°æ®å¢å¼º 
# ã€æ•°æ®å¢å¼ºï¼ï¼è®¾ç½®å¥½è¾“å…¥è¾“å‡ºçš„dataå’Œlabelè·¯å¾„ä¹‹åï¼Œå®ç°æ•°æ®çš„å¢å¼ºå’Œå­˜å‚¨ã€‘
def augment_data(train_data_path, train_labels_path, output_data_path, output_labels_path, noise_std=0.05, plot_channel=10):

    # åŠ è½½è®­ç»ƒæ•°æ®å’Œæ ‡ç­¾
    train_data = np.load(train_data_path)  # shape: (N, 64, 512)
    train_labels = np.load(train_labels_path)

    # åˆå§‹åŒ–å¢å¼ºå®¹å™¨
    data_augmented = []
    labels_augmented = []

    print("ğŸ”§ æ­£åœ¨æŒ‰ trial æ‰©å±•æ¯ä¸ªæ ·æœ¬ï¼ˆä¸‰å€å¢å¼ºï¼‰")

    for i in range(len(train_data)):
        original = train_data[i]
        label = train_labels[i]

        # åŸå§‹æ•°æ®
        data_augmented.append(original)
        labels_augmented.append(label)

        # åŠ æ€§å™ªå£°
        noisy = original + np.random.normal(0, noise_std, size=original.shape).astype(np.float32)
        data_augmented.append(noisy)
        labels_augmented.append(label)

        if i < 2:
            print(f"âœ… Trial {i+1}: å·²ç”ŸæˆåŸå§‹+å™ªå£°å…±2ä»½")

    # è½¬æ¢ä¸º numpy æ ¼å¼
    data_augmented = np.stack(data_augmented)
    labels_augmented = np.array(labels_augmented)

    print(f"ğŸ‰ åŸå§‹æ ·æœ¬æ•°: {len(train_data)} â†’ å¢å¼ºåæ ·æœ¬æ•°: {len(data_augmented)}")

    # ä¿å­˜å¢å¼ºæ•°æ®å’Œæ ‡ç­¾
    os.makedirs(os.path.dirname(output_data_path), exist_ok=True)
    np.save(output_data_path, data_augmented)
    np.save(output_labels_path, labels_augmented)
    print("ğŸ“ å·²ä¿å­˜ä¸‰å€å¢å¼ºæ•°æ®ä¸æ ‡ç­¾")

    # å¯è§†åŒ–æ ·æœ¬å¢å¼ºæ•ˆæœï¼ˆé€šé“ 10ï¼‰
    plt.figure(figsize=(12, 6))
    plt.subplot(3, 1, 1)
    plt.plot(data_augmented[0][plot_channel])
    plt.title("åŸå§‹æ ·æœ¬")

    plt.subplot(3, 1, 2)
    plt.plot(data_augmented[1][plot_channel])
    plt.title("åŠ æ€§å™ªå£°æ ·æœ¬")
    
    plt.tight_layout()
    plt.show()

# %% è®­ç»ƒç›¸å…³
# è®¾ç½®éšæœºç§å­
def set_seed(seed=42):
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

# %% ç‰¹å¾æå– å°æ³¢
def extract_wavelet_features(data, wavelet='db4', level=2):
    n_trials, n_channels, n_samples = data.shape
    features = []
    for trial in data:
        trial_features = []
        for ch_signal in trial:
            coeffs = pywt.wavedec(ch_signal, wavelet=wavelet, level=level)
            feature = np.concatenate([coeffs[0], coeffs[1]])  # cA2 + cD2
            trial_features.append(feature)
        features.append(trial_features)
    return np.array(features)

# %% LSTMæ¨¡å‹ç›¸å…³ï¼ˆç»“æ„ä¸åˆå§‹åŒ–ï¼‰
'''åˆå§‹åŒ–+forward
ä½¿ç”¨LSTMç½‘ç»œå¤„ç†æ—¶é—´åºåˆ—
ä¸¤å±‚å…¨è¿æ¥è¿›è¡Œåˆ†ç±»
ä½¿ç”¨Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
'''
class EEGWaveletLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, num_layers=2, num_classes=2, dropout=0.7):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, num_classes)
        )

    def forward(self, x):
        out, _ = self.lstm(x)
        out = out[:, -1, :]
        return self.classifier(out)

# åˆå§‹åŒ–æƒé‡
def init_weights(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight)
        if m.bias is not None:
            nn.init.zeros_(m.bias)
    elif isinstance(m, nn.LSTM):
        for name, param in m.named_parameters():
            if 'weight_ih' in name:
                nn.init.xavier_uniform_(param.data)
            elif 'weight_hh' in name:
                nn.init.orthogonal_(param.data)
            elif 'bias' in name:
                nn.init.constant_(param.data, 0)
# %% train_model æ¨¡å‹è®­ç»ƒ
# è®­ç»ƒæµç¨‹ è¿”å›ä¸€ä¸ªmodel å’Œbest_acc
# ä¸€ä¸ªtrain_modelä¸€ä¸ªäºŒåˆ†ç±»
def train_model_save(model, train_loader, val_loader, device, epochs=1000, patience=50, model_save_path="best_model.pth", progress_callback = None):
    model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=5e-5)
    criterion = nn.CrossEntropyLoss()
    train_loss_list = []
    val_acc_list = []

    best_acc = 0
    best_state = None
    patience_counter = 0

    for epoch in range(epochs):
        progress = int((epoch + 1) * 100 / epochs)
        model.train()
        total_train_loss = 0
        for xb, yb in train_loader:
            xb, yb = xb.to(device), yb.to(device)
            optimizer.zero_grad()
            loss = criterion(model(xb), yb)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)  # æ¢¯åº¦è£å‰ª
            optimizer.step()
            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)
        train_loss_list.append(avg_train_loss)

        model.eval()
        y_true, y_pred = [], []
        with torch.no_grad():
            for xb, yb in val_loader:
                xb = xb.to(device)
                preds = model(xb).argmax(1).cpu().numpy()
                y_true.extend(yb.numpy())
                y_pred.extend(preds)

        acc = accuracy_score(y_true, y_pred)
        val_acc_list.append(acc)
        print(f"Epoch {epoch+1}/{epochs} - Val Accuracy: {acc:.4f} - Train Loss: {avg_train_loss:.4f}")

        # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
        if progress_callback:
            message = (f"Epoch {epoch+1}/{epochs}\n"
                      f"Validation Accuracy: {acc:.4f}\n"
                      f"Training Loss: {avg_train_loss:.4f}")
            progress_callback(progress, message)


        if acc > best_acc:
            best_acc = acc
            best_state = model.state_dict()
            torch.save(best_state, model_save_path)  # ä¿å­˜æœ€ä½³æ¨¡å‹çš„å‚æ•°
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                if progress_callback:
                    progress_callback(100, "Early stopping triggered!")
                break

    if best_state:
        model.load_state_dict(best_state)

    return model, best_acc

def train_model(model, train_loader, val_loader, device, epochs=1000, patience=5, progress_callback = None):
    model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=5e-5)
    criterion = nn.CrossEntropyLoss()
    
    # ç”¨æ¥ä¿å­˜æ¯ä¸ªepochçš„æŸå¤±å’ŒéªŒè¯å‡†ç¡®ç‡
    train_loss_list = []
    val_acc_list = []
    best_acc = 0
    best_state = None
    patience_counter = 0

    # best performance
    best_performance = {
        'train_loss':[],
        'val_acc':[],
        'final_confusion_matrix': None,
        'y_true': None,
        'y_pred': None
    }

    for epoch in range(epochs):
        progress = int((epoch+1)* 100 / epochs)
        model.train()
        total_train_loss = 0  # ç”¨äºè®°å½•ä¸€ä¸ªepochçš„è®­ç»ƒæŸå¤±
        
        for xb, yb in train_loader:
            xb, yb = xb.to(device), yb.to(device)
            optimizer.zero_grad()
            loss = criterion(model(xb), yb)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)  # æ¢¯åº¦è£å‰ª
            optimizer.step()

            total_train_loss += loss.item() * xb.size(0)  # ç´¯åŠ æ¯ä¸ªbatchçš„æŸå¤±

        avg_train_loss = total_train_loss / len(train_loader.dataset)  # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
        train_loss_list.append(avg_train_loss)  # ä¿å­˜è¯¥epochçš„è®­ç»ƒæŸå¤±

        # è¯„ä¼°é˜¶æ®µ
        model.eval()
        y_true, y_pred = [], []
        with torch.no_grad():
            for xb, yb in val_loader:
                xb = xb.to(device)
                preds = model(xb).argmax(1).cpu().numpy()
                y_true.extend(yb.numpy())
                y_pred.extend(preds)

        acc = accuracy_score(y_true, y_pred)
        val_acc_list.append(acc)
        print(f"Epoch {epoch+1}/{epochs} - Val Accuracy: {acc:.4f} - Train Loss: {avg_train_loss:.4f}")

        # æ›´æ–°è¿›åº¦ä¿¡æ¯
        if progress_callback:
            message = (f"Epoch {epoch+1}/{epochs}\n"
                      f"Validation Accuracy: {acc:.4f}\n"
                      f"Training Loss: {avg_train_loss:.4f}")
            progress_callback(progress, message)

        if acc > best_acc:
            best_acc = acc
            best_state = model.state_dict()
            # ä¿å­˜æœ€ä½³çš„æ€§èƒ½æŒ‡æ ‡
            best_performance['train_loss'] = train_loss_list
            best_performance['val_acc'] = val_acc_list
            best_performance['final_confusion_matrix'] = confusion_matrix(y_true=y_true, y_pred=y_pred)
            best_performance['y_true'] = y_true
            best_performance['y_pred'] = y_pred

            # torch.save(best_state, "best_model_3fenlei_01.pth")  # âœ… ä¿å­˜æ¨¡å‹å‚æ•°
            print(f"Best model saved with accuracy: {best_acc:.4f}")
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print("â¹ï¸ Early stopping triggered")
                break

    if best_state:
        model.load_state_dict(best_state)

    # è®­ç»ƒå®Œæˆæ—¶çš„æœ€ç»ˆè¿›åº¦æ›´æ–°
    if progress_callback:
        progress_callback(100, "Training completed!\nClick OK to continue...")


    return model, best_acc, best_performance

# %% æµ‹è¯•é›†é¢„å¤„ç†
def preprocess_file_test(data_path,  out_data_path,  fs=256, lowcut=0.5, highcut=50.0):
    
    data = data_path  # (n_trials, C, T)

    b, a = butter(N=4, Wn=[lowcut / (fs / 2), highcut / (fs / 2)], btype='band')
    start_idx, end_idx = int(1.0 * fs), int(3.0 * fs)

    n_trials, C, T = data.shape
    processed = np.zeros((n_trials, C, end_idx - start_idx), dtype=np.float32)

    for i in range(n_trials):
        print(f"é¢„å¤„ç†ç¬¬ {i+1}/{n_trials} ä¸ªè¯•æ¬¡...")
        segment = data[i][:, start_idx:end_idx]
        filtered = filtfilt(b, a, segment, axis=1)
        denoised = np.array([wavelet_denoise(filtered[ch])[:end_idx-start_idx] for ch in range(C)])
        zscored = (denoised - denoised.mean()) / (denoised.std() + 1e-8)
        processed[i] = zscored

    print(f"ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®åˆ°ï¼š{out_data_path}")
    np.save(out_data_path, processed)

    print("å¤„ç†å®Œæˆã€‚\n")
    return processed

# %% 