# %% [markdown]
# # æ‰€æœ‰å‡½æ•°æ”¾è¿™é‡Œ

# %%
import os
import numpy as np
import pywt
import matplotlib.pyplot as plt
from scipy.signal import butter, filtfilt
import os
import numpy as np
import pywt
import matplotlib.pyplot as plt
from scipy.signal import butter, filtfilt
import os
import numpy as np
import pywt
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import seaborn as sns
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
import pandas as pd
import random
import os
import numpy as np
import pywt
import matplotlib.pyplot as plt
from scipy.signal import butter, filtfilt
import os
import numpy as np
import pywt
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import seaborn as sns
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import *
import pandas as pd



plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']  # æŒ‡å®šé»˜è®¤å­—ä½“ä¸ºå¾®è½¯é›…é»‘
plt.rcParams['axes.unicode_minus'] = False  # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·


# %% [markdown]
# ## é¢„å¤„ç†

# %%

# è¤ç«è™«ä¼˜åŒ–ç®—æ³•ï¼ˆä¼˜åŒ–è½¯é˜ˆå€¼ï¼‰â€”â€”ç”¨äºæ•°æ®é¢„å¤„ç†
def firefly_algorithm(detail_coeffs, n_fireflies=15, max_iter=20, alpha=0.2, beta0=1, gamma=1):
    def fitness(threshold):
        thresholded = pywt.threshold(detail_coeffs, threshold, mode='soft')
        return -np.var(thresholded)  # è¶Šå¹³æ»‘è¶Šå¥½ï¼ˆè´Ÿæ–¹å·®ï¼‰

    thresholds = np.random.uniform(0.01, np.max(np.abs(detail_coeffs)), n_fireflies)
    fitness_vals = np.array([fitness(th) for th in thresholds])

    for t in range(max_iter):
        for i in range(n_fireflies):
            for j in range(n_fireflies):
                if fitness_vals[j] > fitness_vals[i]:
                    r = np.abs(thresholds[i] - thresholds[j])
                    beta = beta0 * np.exp(-gamma * r**2)
                    thresholds[i] = thresholds[i] + beta * (thresholds[j] - thresholds[i]) + alpha * (np.random.rand() - 0.5)
        fitness_vals = np.array([fitness(th) for th in thresholds])

    best_idx = np.argmax(fitness_vals)
    return thresholds[best_idx]

# å°æ³¢å»å™ªï¼ˆæ¯ä¸ªé€šé“åº”ç”¨ FOA-soft é˜ˆå€¼ï¼‰
def wavelet_denoise(signal, wavelet='db4', level=4):
    coeffs = pywt.wavedec(signal, wavelet, level=level)
    thresholded_coeffs = [coeffs[0]]
    for detail in coeffs[1:]:
        th = firefly_algorithm(detail)
        thresholded = pywt.threshold(detail, th, mode='soft')
        thresholded_coeffs.append(thresholded)
    return pywt.waverec(thresholded_coeffs, wavelet)

# EEG ä¿¡å·é¢„å¤„ç†å‡½æ•°ï¼ˆæ»¤æ³¢ + FOA-DWT å»å™ª + Z-scoreï¼‰
# ä¿®æ”¹ preprocess_file å‡½æ•°ä»¥æ·»åŠ æ‰“å°è¿‡ç¨‹ä¸æ›´å¤šå¯è§†åŒ–ï¼Œè¿”å›çš„æ˜¯processedçš„æ•°æ®å’Œlabels
# EEG ä¿¡å·é¢„å¤„ç†å‡½æ•°ï¼ˆæ»¤æ³¢ + FOA-DWT å»å™ª + Z-scoreï¼‰
def preprocess_file(data_path, label_path, out_data_path, out_label_path, labels_to_select, label_mapping, fs=256, lowcut=0.5, highcut=50.0):
    print(f"åŠ è½½æ•°æ®ï¼š{data_path}")
    data = np.load(data_path)  # (n_trials, C, T)
    labels = np.load(label_path)
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶ï¼š{data.shape}ï¼Œæ ‡ç­¾å½¢çŠ¶ï¼š{labels.shape}")

    # ===== æ ‡ç­¾ç­›é€‰ =====
    selected_mask = np.isin(labels, labels_to_select)
    data = data[selected_mask]
    labels = labels[selected_mask]

    # æ ‡ç­¾æ˜ å°„
    labels = np.vectorize(label_mapping.get)(labels)
    print(f"ç­›é€‰åçš„æ ·æœ¬æ•°ï¼š{len(labels)}")

    b, a = butter(N=4, Wn=[lowcut / (fs / 2), highcut / (fs / 2)], btype='band')
    start_idx, end_idx = int(1.0 * fs), int(3.0 * fs)

    n_trials, C, T = data.shape
    processed = np.zeros((n_trials, C, end_idx - start_idx), dtype=np.float32)

    for i in range(n_trials):
        print(f"é¢„å¤„ç†ç¬¬ {i+1}/{n_trials} ä¸ªè¯•æ¬¡...")
        segment = data[i][:, start_idx:end_idx]
        filtered = filtfilt(b, a, segment, axis=1)
        denoised = np.array([wavelet_denoise(filtered[ch])[:end_idx-start_idx] for ch in range(C)])
        zscored = (denoised - denoised.mean()) / (denoised.std() + 1e-8)
        processed[i] = zscored

    print(f"ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®åˆ°ï¼š{out_data_path}")
    np.save(out_data_path, processed)
    np.save(out_label_path, labels)
    print("å¤„ç†å®Œæˆã€‚\n")
    return processed, labels

# å¯è§†åŒ–å‡½æ•°ï¼ˆé»˜è®¤å±•ç¤º Trial 0, Channel 10ï¼‰
def plot_example_signal(data, trial_idx=0, channel_idx=10, title="Preprocessed EEG with FOA-DWT"):
    plt.figure(figsize=(10, 4))
    plt.plot(data[trial_idx, channel_idx], label=f"Trial {trial_idx}, Channel {channel_idx}")
    plt.title(title)
    plt.xlabel("Sample Index")
    plt.ylabel("Amplitude (z-scored)")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# %% [markdown]
# ## æ•°æ®å¢å¼º

# %%
# ã€æ•°æ®å¢å¼ºï¼ï¼è®¾ç½®å¥½è¾“å…¥è¾“å‡ºçš„dataå’Œlabelè·¯å¾„ä¹‹åï¼Œå®ç°æ•°æ®çš„å¢å¼ºå’Œå­˜å‚¨ã€‘
def augment_data(train_data_path, train_labels_path, output_data_path, output_labels_path, noise_std=0.05, plot_channel=10):

    # åŠ è½½è®­ç»ƒæ•°æ®å’Œæ ‡ç­¾
    train_data = np.load(train_data_path)  # shape: (N, 64, 512)
    train_labels = np.load(train_labels_path)

    # åˆå§‹åŒ–å¢å¼ºå®¹å™¨
    data_augmented = []
    labels_augmented = []

    print("ğŸ”§ æ­£åœ¨æŒ‰ trial æ‰©å±•æ¯ä¸ªæ ·æœ¬ï¼ˆä¸‰å€å¢å¼ºï¼‰")

    for i in range(len(train_data)):
        original = train_data[i]
        label = train_labels[i]

        # åŸå§‹æ•°æ®
        data_augmented.append(original)
        labels_augmented.append(label)

        # åŠ æ€§å™ªå£°
        noisy = original + np.random.normal(0, noise_std, size=original.shape).astype(np.float32)
        data_augmented.append(noisy)
        labels_augmented.append(label)

        if i < 2:
            print(f"âœ… Trial {i+1}: å·²ç”ŸæˆåŸå§‹+å™ªå£°å…±2ä»½")

    # è½¬æ¢ä¸º numpy æ ¼å¼
    data_augmented = np.stack(data_augmented)
    labels_augmented = np.array(labels_augmented)

    print(f"ğŸ‰ åŸå§‹æ ·æœ¬æ•°: {len(train_data)} â†’ å¢å¼ºåæ ·æœ¬æ•°: {len(data_augmented)}")

    # ä¿å­˜å¢å¼ºæ•°æ®å’Œæ ‡ç­¾
    os.makedirs(os.path.dirname(output_data_path), exist_ok=True)
    np.save(output_data_path, data_augmented)
    np.save(output_labels_path, labels_augmented)
    print("ğŸ“ å·²ä¿å­˜ä¸‰å€å¢å¼ºæ•°æ®ä¸æ ‡ç­¾")

    # å¯è§†åŒ–æ ·æœ¬å¢å¼ºæ•ˆæœï¼ˆé€šé“ 10ï¼‰
    plt.figure(figsize=(12, 6))
    plt.subplot(3, 1, 1)
    plt.plot(data_augmented[0][plot_channel])
    plt.title("åŸå§‹æ ·æœ¬")

    plt.subplot(3, 1, 2)
    plt.plot(data_augmented[1][plot_channel])
    plt.title("åŠ æ€§å™ªå£°æ ·æœ¬")
    
    plt.tight_layout()
    plt.show()


# %% [markdown]
# ## è®­ç»ƒç›¸å…³

# %%
# è®¾ç½®éšæœºç§å­
def set_seed(seed=42):
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        
# ================== å°æ³¢ç‰¹å¾æå– ===================
def extract_wavelet_features(data, wavelet='db4', level=2):
    n_trials, n_channels, n_samples = data.shape
    features = []
    for trial in data:
        trial_features = []
        for ch_signal in trial:
            coeffs = pywt.wavedec(ch_signal, wavelet=wavelet, level=level)
            feature = np.concatenate([coeffs[0], coeffs[1]])  # cA2 + cD2
            trial_features.append(feature)
        features.append(trial_features)
    return np.array(features)

# ================== LSTM æ¨¡å‹å®šä¹‰ ===================
'''åˆå§‹åŒ–+forward
ä½¿ç”¨LSTMç½‘ç»œå¤„ç†æ—¶é—´åºåˆ—
ä¸¤å±‚å…¨è¿æ¥è¿›è¡Œåˆ†ç±»
ä½¿ç”¨Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
'''
class EEGWaveletLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, num_layers=2, num_classes=2, dropout=0.5):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, num_classes)
        )

    def forward(self, x):
        out, _ = self.lstm(x)
        out = out[:, -1, :]
        return self.classifier(out)
    
# ================== è‡ªå®šä¹‰æƒé‡åˆå§‹åŒ– ===================
def init_weights(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight)
        if m.bias is not None:
            nn.init.zeros_(m.bias)
    elif isinstance(m, nn.LSTM):
        for name, param in m.named_parameters():
            if 'weight_ih' in name:
                nn.init.xavier_uniform_(param.data)
            elif 'weight_hh' in name:
                nn.init.orthogonal_(param.data)
            elif 'bias' in name:
                nn.init.constant_(param.data, 0)

# è®­ç»ƒæµç¨‹ è¿”å›ä¸€ä¸ªmodel å’Œbest_acc
# ä¸€ä¸ªtrain_modelä¸€ä¸ªäºŒåˆ†ç±»
def train_model(model, train_loader, val_loader, device, epochs=1000, patience=50, model_save_path="best_model.pth"):
    model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=5e-5)
    criterion = nn.CrossEntropyLoss()
    train_loss_list = []
    val_acc_list = []

    best_acc = 0
    best_state = None
    patience_counter = 0

    for epoch in range(epochs):
        model.train()
        total_train_loss = 0
        for xb, yb in train_loader:
            xb, yb = xb.to(device), yb.to(device)
            optimizer.zero_grad()
            loss = criterion(model(xb), yb)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)  # æ¢¯åº¦è£å‰ª
            optimizer.step()
            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)
        train_loss_list.append(avg_train_loss)

        model.eval()
        y_true, y_pred = [], []
        with torch.no_grad():
            for xb, yb in val_loader:
                xb = xb.to(device)
                preds = model(xb).argmax(1).cpu().numpy()
                y_true.extend(yb.numpy())
                y_pred.extend(preds)

        acc = accuracy_score(y_true, y_pred)
        val_acc_list.append(acc)
        print(f"Epoch {epoch+1}/{epochs} - Val Accuracy: {acc:.4f} - Train Loss: {avg_train_loss:.4f}")

        if acc > best_acc:
            best_acc = acc
            best_state = model.state_dict()
            torch.save(best_state, model_save_path)  # âœ… ä¿å­˜æ¨¡å‹å‚æ•°
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print("â¹ï¸ Early stopping triggered")
                break

    if best_state:
        model.load_state_dict(best_state)

    '''
    # ç»˜åˆ¶è®­ç»ƒæŸå¤±å’ŒéªŒè¯å‡†ç¡®ç‡æ›²çº¿
    plt.figure()
    plt.plot(train_loss_list, label="Train Loss", marker='o')
    plt.title("Loss Curve")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    plt.figure()
    plt.plot(val_acc_list, marker='o')
    plt.title("Validation Accuracy Curve")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.tight_layout()
    plt.show()
    '''

    return model, best_acc


# %% [markdown]
# ## æµ‹è¯•é›†

# %%
def preprocess_file_test(data_path,  out_data_path,  fs=256, lowcut=0.5, highcut=50.0):
    
    data = data_path  # (n_trials, C, T)

    b, a = butter(N=4, Wn=[lowcut / (fs / 2), highcut / (fs / 2)], btype='band')
    start_idx, end_idx = int(1.0 * fs), int(3.0 * fs)

    n_trials, C, T = data.shape
    processed = np.zeros((n_trials, C, end_idx - start_idx), dtype=np.float32)

    for i in range(n_trials):
        print(f"é¢„å¤„ç†ç¬¬ {i+1}/{n_trials} ä¸ªè¯•æ¬¡...")
        segment = data[i][:, start_idx:end_idx]
        filtered = filtfilt(b, a, segment, axis=1)
        denoised = np.array([wavelet_denoise(filtered[ch])[:end_idx-start_idx] for ch in range(C)])
        zscored = (denoised - denoised.mean()) / (denoised.std() + 1e-8)
        processed[i] = zscored

    print(f"ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®åˆ°ï¼š{out_data_path}")
    np.save(out_data_path, processed)

    print("å¤„ç†å®Œæˆã€‚\n")
    return processed

# %% [markdown]
# # å¼€å§‹å§

# %% [markdown]
# ## ç”Ÿæˆé¢„å¤„ç†ä¹‹åçš„æ•°æ®

# %%
# åˆ›å»ºä¿å­˜è·¯å¾„
os.makedirs("Training_Sample01_preprocess", exist_ok=True)
os.makedirs("Validation_Sample01_preprocess", exist_ok=True)
os.makedirs("Test_Sample01_preprocess", exist_ok=True)
# ç”Ÿæˆä¸åŒçš„äºŒåˆ†ç±»æ¨¡å‹æ•°æ®é›†

# ç”Ÿæˆ 0-1 ç±»åˆ«çš„æ•°æ®é›†
train_data_0_1, train_labels_0_1 = preprocess_file(
    "Training_set_npy_3fenlei/Data_Sample01_data.npy",
    "Training_set_npy_3fenlei/Data_Sample01_labels.npy",
    "Training_Sample01_preprocess/Data_Sample01_data_pre_0_1.npy",
    "Training_Sample01_preprocess/Data_Sample01_labels_pre_0_1.npy",
    labels_to_select=[0, 2],
    label_mapping={0: 0, 2: 1}
)

val_data_0_1, val_labels_0_1 = preprocess_file(
    "Validation_set_npy_3fenlei/Data_Sample01_data.npy",
    "Validation_set_npy_3fenlei/Data_Sample01_labels.npy",
    "Validation_Sample01_preprocess/Data_Sample01_data_pre_0_1.npy",
    "Validation_Sample01_preprocess/Data_Sample01_labels_pre_0_1.npy",
    labels_to_select=[0, 2],
    label_mapping={0: 0, 2: 1}
)

# ç”Ÿæˆ 0-2 ç±»åˆ«çš„æ•°æ®é›†
train_data_0_2, train_labels_0_2 = preprocess_file(
    "Training_set_npy_3fenlei/Data_Sample01_data.npy",
    "Training_set_npy_3fenlei/Data_Sample01_labels.npy",
    "Training_Sample01_preprocess/Data_Sample01_data_pre_0_2.npy",
    "Training_Sample01_preprocess/Data_Sample01_labels_pre_0_2.npy",
    labels_to_select=[0, 4],
    label_mapping={0: 0, 4: 1}
)

val_data_0_2, val_labels_0_2 = preprocess_file(
    "Validation_set_npy_3fenlei/Data_Sample01_data.npy",
    "Validation_set_npy_3fenlei/Data_Sample01_labels.npy",
    "Validation_Sample01_preprocess/Data_Sample01_data_pre_0_2.npy",
    "Validation_Sample01_preprocess/Data_Sample01_labels_pre_0_2.npy",
    labels_to_select=[0, 4],
    label_mapping={0: 0, 4: 1}
)

# ç”Ÿæˆ 1-2 ç±»åˆ«çš„æ•°æ®é›†
train_data_1_2, train_labels_1_2 = preprocess_file(
    "Training_set_npy_3fenlei/Data_Sample01_data.npy",
    "Training_set_npy_3fenlei/Data_Sample01_labels.npy",
    "Training_Sample01_preprocess/Data_Sample01_data_pre_1_2.npy",
    "Training_Sample01_preprocess/Data_Sample01_labels_pre_1_2.npy",
    labels_to_select=[2, 4],
    label_mapping={2: 0, 4: 1}
)

val_data_1_2, val_labels_1_2 = preprocess_file(
    "Validation_set_npy_3fenlei/Data_Sample01_data.npy",
    "Validation_set_npy_3fenlei/Data_Sample01_labels.npy",
    "Validation_Sample01_preprocess/Data_Sample01_data_pre_1_2.npy",
    "Validation_Sample01_preprocess/Data_Sample01_labels_pre_1_2.npy",
    labels_to_select=[2, 4],
    label_mapping={2: 0, 4: 1}
)

test_data = preprocess_file_test(
    "Test_set_npy/Data_Sample01_data.npy",
    "Test_Sample01_preprocess/Data_Sample01_data_pre.npy",
)

# å¯è§†åŒ–é¢„å¤„ç†åçš„æ ·æœ¬ï¼ˆä¾‹å¦‚0-1æ•°æ®é›†ï¼‰
plot_example_signal(val_data_0_1)

# %% [markdown]
# ## æ•°æ®å¢å¼º åŸºäºå‰é¢çš„preprocessç”Ÿæˆ

# %%
augment_data(
    train_data_path="Training_Sample01_preprocess/Data_Sample01_data_pre_0_1.npy",
    train_labels_path="Training_Sample01_preprocess/Data_Sample01_labels_pre_0_1.npy",
    output_data_path="Training_Sample01_preprocess/Data_Sample01_data_aug_0_1.npy",
    output_labels_path="Training_Sample01_preprocess/Data_Sample01_labels_aug_0_1.npy"
)

augment_data(
    train_data_path="Training_Sample01_preprocess/Data_Sample01_data_pre_0_2.npy",
    train_labels_path="Training_Sample01_preprocess/Data_Sample01_labels_pre_0_2.npy",
    output_data_path="Training_Sample01_preprocess/Data_Sample01_data_aug_0_2.npy",
    output_labels_path="Training_Sample01_preprocess/Data_Sample01_labels_aug_0_2.npy"
)

augment_data(
    train_data_path="Training_Sample01_preprocess/Data_Sample01_data_pre_1_2.npy",
    train_labels_path="Training_Sample01_preprocess/Data_Sample01_labels_pre_1_2.npy",
    output_data_path="Training_Sample01_preprocess/Data_Sample01_data_aug_1_2.npy",
    output_labels_path="Training_Sample01_preprocess/Data_Sample01_labels_aug_1_2.npy"
)

# %% [markdown]
# ## å…ˆè·å¾—æœ€ä½³ç§å­æ•°

# %%
# ================== å°è¯•å¤šä¸ªç§å­å¹¶è¾“å‡ºæœ€ç»ˆå‡†ç¡®ç‡ ===================
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# åŠ è½½æ•°æ®
train_data_0_1 = np.load("Training_Sample01_preprocess/Data_Sample01_data_aug_0_1.npy")
train_labels_0_1 = np.load("Training_Sample01_preprocess/Data_Sample01_labels_aug_0_1.npy")
train_data_0_2 = np.load("Training_Sample01_preprocess/Data_Sample01_data_aug_0_2.npy")
train_labels_0_2 = np.load("Training_Sample01_preprocess/Data_Sample01_labels_aug_0_2.npy")
train_data_1_2 = np.load("Training_Sample01_preprocess/Data_Sample01_data_aug_1_2.npy")
train_labels_1_2 = np.load("Training_Sample01_preprocess/Data_Sample01_labels_aug_1_2.npy")

val_data_0_1 = np.load("Validation_Sample01_preprocess/Data_Sample01_data_pre_0_1.npy")
val_labels_0_1 = np.load("Validation_Sample01_preprocess/Data_Sample01_labels_pre_0_1.npy")
val_data_0_2 = np.load("Validation_Sample01_preprocess/Data_Sample01_data_pre_0_2.npy")
val_labels_0_2 = np.load("Validation_Sample01_preprocess/Data_Sample01_labels_pre_0_2.npy")
val_data_1_2 = np.load("Validation_Sample01_preprocess/Data_Sample01_data_pre_1_2.npy")
val_labels_1_2 = np.load("Validation_Sample01_preprocess/Data_Sample01_labels_pre_1_2.npy")

# æå–ç‰¹å¾
train_features_0_1 = extract_wavelet_features(train_data_0_1)
val_features_0_1 = extract_wavelet_features(val_data_0_1)
train_features_0_2 = extract_wavelet_features(train_data_0_2)
val_features_0_2 = extract_wavelet_features(val_data_0_2)
train_features_1_2 = extract_wavelet_features(train_data_1_2)
val_features_1_2 = extract_wavelet_features(val_data_1_2)




# %%
# è®­ç»ƒæ¯ä¸ªæ¨¡å‹
accuracies_0_1 = []
best_seed = None
best_acc = 0
best_model = None
for seed in range(0, 351):  # é€‰æ‹©ç§å­èŒƒå›´ 0-350
    set_seed(seed)
    model = EEGWaveletLSTM(input_dim=train_features_0_1.shape[2])
    model.apply(init_weights)
    train_loader = DataLoader(TensorDataset(torch.tensor(train_features_0_1, dtype=torch.float32), torch.tensor(train_labels_0_1, dtype=torch.long)), batch_size=64, shuffle=True)
    val_loader = DataLoader(TensorDataset(torch.tensor(val_features_0_1, dtype=torch.float32), torch.tensor(val_labels_0_1, dtype=torch.long)), batch_size=64)

    trained_model, final_acc = train_model(model, train_loader, val_loader, device, epochs=1000, patience=50)# model_save_path="best_model_0_1_seed_{seed}.pth"
    accuracies_0_1.append((seed, final_acc))

    # ä¿å­˜å½“å‰æœ€å¥½çš„æ¨¡å‹
    if final_acc > best_acc:
        best_acc = final_acc
        best_seed = seed
        best_model = trained_model

# è¾“å‡ºTop 5å‡†ç¡®ç‡çš„ç§å­
accuracies_0_1.sort(key=lambda x: x[1], reverse=True)
print("\nTop 5 Seed Accuracies (0-1):")
for seed, acc in accuracies_0_1[:5]:
    print(f"Seed: {seed}, Accuracy: {acc:.4f}")

# åœ¨æ‰¾åˆ°æœ€ä½³ç§å­åä¿å­˜æœ€ä½³æ¨¡å‹
if best_model is not None:
    best_model_save_path = f"best_model_Sample01_0_1.pth"
    torch.save(best_model.state_dict(), best_model_save_path)
    print(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼š{best_model_save_path}")
    
# è®­ç»ƒæ¯ä¸ªæ¨¡å‹ï¼ˆ0-2ï¼‰
accuracies_0_2 = []
best_seed_0_2 = None
best_acc_0_2 = 0
best_model_0_2 = None
for seed in range(0, 351):  # é€‰æ‹©ç§å­èŒƒå›´ 0-350
    set_seed(seed)
    model = EEGWaveletLSTM(input_dim=train_features_0_2.shape[2])
    model.apply(init_weights)
    train_loader = DataLoader(TensorDataset(torch.tensor(train_features_0_2, dtype=torch.float32), torch.tensor(train_labels_0_2, dtype=torch.long)), batch_size=64, shuffle=True)
    val_loader = DataLoader(TensorDataset(torch.tensor(val_features_0_2, dtype=torch.float32), torch.tensor(val_labels_0_2, dtype=torch.long)), batch_size=64)

    trained_model, final_acc = train_model(model, train_loader, val_loader, device, epochs=1000, patience=50)
    accuracies_0_2.append((seed, final_acc))

    # ä¿å­˜å½“å‰æœ€å¥½çš„æ¨¡å‹
    if final_acc > best_acc_0_2:
        best_acc_0_2 = final_acc
        best_seed_0_2 = seed
        best_model_0_2 = trained_model

# è¾“å‡ºTop 5å‡†ç¡®ç‡çš„ç§å­ï¼ˆ0-2ï¼‰
accuracies_0_2.sort(key=lambda x: x[1], reverse=True)
print("\nTop 5 Seed Accuracies (0-2):")
for seed, acc in accuracies_0_2[:5]:
    print(f"Seed: {seed}, Accuracy: {acc:.4f}")

# åœ¨æ‰¾åˆ°æœ€ä½³ç§å­åä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆ0-2ï¼‰
if best_model_0_2 is not None:
    best_model_save_path_0_2 = f"best_model_Sample01_0_2.pth"
    torch.save(best_model_0_2.state_dict(), best_model_save_path_0_2)
    print(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼š{best_model_save_path_0_2}")


# è®­ç»ƒæ¯ä¸ªæ¨¡å‹ï¼ˆ1-2ï¼‰
accuracies_1_2 = []
best_seed_1_2 = None
best_acc_1_2 = 0
best_model_1_2 = None
for seed in range(0, 351):  # é€‰æ‹©ç§å­èŒƒå›´ 0-350
    set_seed(seed)
    model = EEGWaveletLSTM(input_dim=train_features_1_2.shape[2])
    model.apply(init_weights)
    train_loader = DataLoader(TensorDataset(torch.tensor(train_features_1_2, dtype=torch.float32), torch.tensor(train_labels_1_2, dtype=torch.long)), batch_size=64, shuffle=True)
    val_loader = DataLoader(TensorDataset(torch.tensor(val_features_1_2, dtype=torch.float32), torch.tensor(val_labels_1_2, dtype=torch.long)), batch_size=64)

    trained_model, final_acc = train_model(model, train_loader, val_loader, device, epochs=1000, patience=50)
    accuracies_1_2.append((seed, final_acc))

    # ä¿å­˜å½“å‰æœ€å¥½çš„æ¨¡å‹
    if final_acc > best_acc_1_2:
        best_acc_1_2 = final_acc
        best_seed_1_2 = seed
        best_model_1_2 = trained_model

# è¾“å‡ºTop 5å‡†ç¡®ç‡çš„ç§å­ï¼ˆ1-2ï¼‰
accuracies_1_2.sort(key=lambda x: x[1], reverse=True)
print("\nTop 5 Seed Accuracies (1-2):")
for seed, acc in accuracies_1_2[:5]:
    print(f"Seed: {seed}, Accuracy: {acc:.4f}")

# åœ¨æ‰¾åˆ°æœ€ä½³ç§å­åä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆ1-2ï¼‰
if best_model_1_2 is not None:
    best_model_save_path_1_2 = f"best_model_Sample01_1_2.pth"
    torch.save(best_model_1_2.state_dict(), best_model_save_path_1_2)
    print(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼š{best_model_save_path_1_2}")

# %% [markdown]
# ## å¸¦ç»“æœçš„è®­ç»ƒ

# %%
# =============== 7. è®­ç»ƒå‡½æ•°ï¼ˆåŠ æ—©åœæœºåˆ¶ + æ¢¯åº¦è£å‰ªï¼‰åªæ˜¯æ²¡æœ‰æ¨¡å‹çš„ä¿å­˜ä½ç½® ===============
def train_model(model, train_loader, val_loader, device, epochs=1000, patience=50):
    model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=5e-5)
    criterion = nn.CrossEntropyLoss()
    
    # ç”¨æ¥ä¿å­˜æ¯ä¸ªepochçš„æŸå¤±å’ŒéªŒè¯å‡†ç¡®ç‡
    train_loss_list = []
    val_acc_list = []

    best_acc = 0
    best_state = None
    patience_counter = 0

    for epoch in range(epochs):
        model.train()
        total_train_loss = 0  # ç”¨äºè®°å½•ä¸€ä¸ªepochçš„è®­ç»ƒæŸå¤±
        
        for xb, yb in train_loader:
            xb, yb = xb.to(device), yb.to(device)
            optimizer.zero_grad()
            loss = criterion(model(xb), yb)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)  # æ¢¯åº¦è£å‰ª
            optimizer.step()

            total_train_loss += loss.item() * xb.size(0)  # ç´¯åŠ æ¯ä¸ªbatchçš„æŸå¤±

        avg_train_loss = total_train_loss / len(train_loader.dataset)  # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
        train_loss_list.append(avg_train_loss)  # ä¿å­˜è¯¥epochçš„è®­ç»ƒæŸå¤±

        # è¯„ä¼°é˜¶æ®µ
        model.eval()
        y_true, y_pred = [], []
        with torch.no_grad():
            for xb, yb in val_loader:
                xb = xb.to(device)
                preds = model(xb).argmax(1).cpu().numpy()
                y_true.extend(yb.numpy())
                y_pred.extend(preds)

        acc = accuracy_score(y_true, y_pred)
        val_acc_list.append(acc)
        print(f"Epoch {epoch+1}/{epochs} - Val Accuracy: {acc:.4f} - Train Loss: {avg_train_loss:.4f}")

        if acc > best_acc:
            best_acc = acc
            best_state = model.state_dict()
            # torch.save(best_state, "best_model_3fenlei_01.pth")  # âœ… ä¿å­˜æ¨¡å‹å‚æ•°
            print(f"Best model saved with accuracy: {best_acc:.4f}")
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print("â¹ï¸ Early stopping triggered")
                break

    if best_state:
        model.load_state_dict(best_state)

    # ç»˜åˆ¶éªŒè¯å‡†ç¡®ç‡æ›²çº¿
    plt.figure()
    plt.plot(val_acc_list, marker='o')
    plt.title("Validation Accuracy Curve")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.grid(True)
    plt.tight_layout()
    plt.show()
    
    # ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿
    plt.figure()
    plt.plot(train_loss_list, label="Train Loss", marker='o')
    plt.title("Train Loss Curve")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.tight_layout()
    plt.show()

    return model



# %%
print(best_seed, best_acc, best_seed_0_2, best_acc_0_2, best_seed_1_2, best_acc_1_2)

# %%
best_seed = 211
best_seed_0_2 = 102
best_seed_1_2 = 121

set_seed(best_seed)
model = EEGWaveletLSTM(input_dim=train_features_0_1.shape[2])
model.apply(init_weights)
train_loader = DataLoader(TensorDataset(torch.tensor(train_features_0_1, dtype=torch.float32), torch.tensor(train_labels_0_1, dtype=torch.long)), batch_size=64, shuffle=True)
val_loader = DataLoader(TensorDataset(torch.tensor(val_features_0_1, dtype=torch.float32), torch.tensor(val_labels_0_1, dtype=torch.long)), batch_size=64)

trained_model = train_model(model, train_loader, val_loader, device, epochs=1000, patience=50)# model_save_path="best_model_0_1_seed_{seed}.pth"


# %%
best_seed = 211
best_seed_0_2 = 102
best_seed_1_2 = 121

set_seed(best_seed_0_2)
model = EEGWaveletLSTM(input_dim=train_features_0_2.shape[2])
model.apply(init_weights)
train_loader = DataLoader(TensorDataset(torch.tensor(train_features_0_2, dtype=torch.float32), torch.tensor(train_labels_0_2, dtype=torch.long)), batch_size=64, shuffle=True)
val_loader = DataLoader(TensorDataset(torch.tensor(val_features_0_2, dtype=torch.float32), torch.tensor(val_labels_0_2, dtype=torch.long)), batch_size=64)

trained_model = train_model(model, train_loader, val_loader, device, epochs=1000, patience=50)# model_save_path="best_model_0_1_seed_{seed}.pth"


# %%
best_seed = 211
best_seed_0_2 = 102
best_seed_1_2 = 121

set_seed(best_seed_1_2)
model = EEGWaveletLSTM(input_dim=train_features_1_2.shape[2])
model.apply(init_weights)
train_loader = DataLoader(TensorDataset(torch.tensor(train_features_1_2, dtype=torch.float32), torch.tensor(train_labels_1_2, dtype=torch.long)), batch_size=64, shuffle=True)
val_loader = DataLoader(TensorDataset(torch.tensor(val_features_1_2, dtype=torch.float32), torch.tensor(val_labels_1_2, dtype=torch.long)), batch_size=64)

trained_model = train_model(model, train_loader, val_loader, device, epochs=1000, patience=50)# model_save_path="best_model_0_1_seed_{seed}.pth"


# %% [markdown]
# ## ä½¿ç”¨è¿™ä¸‰ä¸ªæ¨¡å‹

test_data =  np.load("Test_Sample01_preprocess/Data_Sample01_data_pre.npy") 
# 3. å°æ³¢ç‰¹å¾æå–
test_features = extract_wavelet_features(test_data)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 4. æ„é€ æ¨¡å‹è¾“å…¥å¹¶é¢„æµ‹
test_X = torch.tensor(test_features, dtype=torch.float32).to(device)

temp_input_dim = test_X.shape[2]
test_model = EEGWaveletLSTM(input_dim=temp_input_dim)
test_model.load_state_dict(torch.load("best_model_Sample01_0_1.pth", map_location=device))
test_model.to(device)
test_model.eval()

with torch.no_grad():
    outputs = test_model(test_X)
    predictions = outputs.argmax(dim=1).cpu().numpy()
    probs = torch.softmax(outputs, dim=1).cpu().numpy()

'''
# 5. æ‰“å°é¢„æµ‹ç»“æœ
for i, (pred, prob) in enumerate(zip(predictions, probs)):
    print(f"ğŸ§ª Trial {i}: Predicted Class = {pred}, Probabilities = {np.round(prob, 3)}")

# 6. ï¼ˆå¯é€‰ï¼‰è®¡ç®—å‡†ç¡®ç‡
if 'test_labels' in locals():
    test_acc = accuracy_score(test_labels, predictions)
    print(f"âœ… æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}")
'''

# ========== è¯»å–äººå·¥æ ‡æ³¨æ ‡ç­¾ï¼ˆä»å›¾ä¸­æ•´ç†ï¼‰ ==========
manual_true_labels = [
    4,1,5,3,2,5,5,3,2,4,4,4,2,4,3,5,2,2,4,3,
    3,4,1,3,4,4,5,1,2,1,3,3,5,2,1,1,2,5,1,1,
    5,3,4,5,1,5,2,2,3,1
]

true_labels = np.array(manual_true_labels) - 1  # è½¬ä¸º0-based

# ä½¿ç”¨å‰é¢è”åˆæ¨¡å‹çš„ final_preds ä½œä¸º predictions
predictions = predictions  # è¿™åœ¨å‰é¢çš„æ¨ç†æµç¨‹ä¸­åº”å·²ç”Ÿæˆ

# ä»…ä¿ç•™æ ‡ç­¾ä¸º 0, 2, 4 çš„æ ·æœ¬ç”¨äºè¯„ä¼°
target_labels = {0, 2}
indices_024 = [i for i, label in enumerate(true_labels) if label in target_labels]

print("ä»¥ä¸‹æ˜¯ä»…é’ˆå¯¹å®é™…ä¸º 1/3/5 çš„æ ·æœ¬ï¼š")
print("Trial\tTrue(0/1/2)\tPred(0/1/2)")

filtered_true = true_labels[indices_024]
filtered_pred = predictions[indices_024]



# æ··æ·†çŸ©é˜µï¼ˆæ ‡ç­¾æ˜ å°„ä¸º 0/1/2ï¼‰
# ğŸ” å°† true_labels çš„å€¼ [0,2,4] â†’ æ˜ å°„åˆ° [0,1,2]
label_mapping = {0: 0, 2: 1}  # æ˜¾å¼æ˜ å°„
mapped_true = np.array([label_mapping[label] for label in filtered_true])
mapped_pred = filtered_pred  # ä½ çš„æ¨¡å‹å·²ç»æ˜¯ 0/1/2 è¾“å‡ºï¼Œç›´æ¥ç”¨

# è®¡ç®—å‡†ç¡®ç‡ï¼ˆä»…å¯¹0/2/4ï¼‰
acc_024 = accuracy_score(mapped_true, mapped_pred)
print(f"ä»…å¯¹æ ‡ç­¾0/2çš„é¢„æµ‹å‡†ç¡®ç‡ï¼š{acc_024:.4f}")

for idx_in_filtered, idx_original in enumerate(indices_024):
    true_label = mapped_true[idx_in_filtered]
    pred_label = mapped_pred[idx_in_filtered]
    print(f"{idx_original:5d}\t{true_label}\t\t{pred_label}")
    

# æ··æ·†çŸ©é˜µå¯è§†åŒ–
cm = confusion_matrix(mapped_true, mapped_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu',
            xticklabels=[0, 1], yticklabels=[0, 1])
plt.title("Confusion Matrix (labels 0,2,4 only)")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.tight_layout()
plt.show()

# è®¡ç®— F1 åˆ†æ•°
f1_macro = f1_score(mapped_true, mapped_pred, average='macro')
f1_micro = f1_score(mapped_true, mapped_pred, average='micro')
f1_weighted = f1_score(mapped_true, mapped_pred, average='weighted')
f1_per_class = f1_score(mapped_true, mapped_pred, average=None)

report = {
    "Acc (0/2/4)": round(acc_024, 4),
    "F1 Macro": round(f1_macro, 2),
    "F1 Micro": round(f1_micro, 2),
    "F1 Weighted": round(f1_weighted, 2),
    "F1 Per Class (0,2,4)": [round(x, 2) for x in f1_per_class.tolist()]
}
report

